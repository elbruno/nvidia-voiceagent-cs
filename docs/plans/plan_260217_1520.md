# ASR Dimension Mismatch Fix - Summary

**Date:** February 17, 2026  
**Issue:** ONNX Runtime dimension mismatch error in ASR service  
**Error Message:** `Attempting to broadcast an axis by a dimension other than 1. [X] by [Y]`

## Root Cause

The ASR service was passing the **mel-spectrogram frame count** as the `length` parameter to the ONNX model, when it actually expects the **original raw audio sample count**.

### The Problem

```csharp
// ❌ WRONG - This was causing the error
long audioLength = numFrames;  // Mel-spectrogram frame count (~100-200)
```

The Parakeet-TDT encoder model expects:

- `audio_signal`: `[batch, 128, time_frames]` - Mel-spectrogram features
- `length`: `[batch]` - **Original audio sample count** (e.g., 16000 for 1 sec)

### The Solution

```csharp
// ✅ CORRECT - Pass raw audio sample count
long audioLength = audioSamples.Length;  // Original audio samples (e.g., 16000-80000)
```

## Changes Made

### 1. Fixed Length Parameter Calculation

**File:** `NvidiaVoiceAgent.Core/Services/AsrService.cs`

**Key Changes:**

- Changed `audioLength` from `numFrames` (mel frames) to `audioSamples.Length` (raw samples)
- Removed complex subsampling calculations that were incorrectly pre-computing lengths
- Added comprehensive debug logging to track tensor shapes

**Before:**

```csharp
long audioLength = numFrames;  // Wrong: ~100-200 frames
```

**After:**

```csharp
long audioLength = audioSamples.Length;  // Correct: ~16000-80000 samples
```

### 2. Improved Model Input Creation

**Method:** `CreateModelInputs()`

- Now accepts `audioSampleLength` instead of `numFrames`
- Properly handles both `long` and `int` type parameters
- Added detailed logging for each input tensor

### 3. Enhanced Debugging

Added logging to show:

- ONNX model input metadata on load
- Actual tensor shapes being passed
- Audio sample count vs mel frame count
- Input/output parameter types

### 4. Created Comprehensive Unit Tests

**File:** `tests/NvidiaVoiceAgent.Core.Tests/AsrServiceTests.cs`

**Test Coverage:**

- ✅ Mock mode functionality
- ✅ Real model loading
- ✅ Various audio durations (0.5s to 5.0s)
- ✅ Edge case audio lengths (specific sample counts)
- ✅ Multiple consecutive calls (stability)
- ✅ Partial transcription with confidence
- ✅ Mel-spectrogram configuration

**Test Results:** 11/11 tests passed ✅

## Validation

### Unit Tests

```bash
dotnet test --filter "AsrServiceTests"
# Result: 11 passed, 0 failed
```

### Integration Tests

```bash
dotnet test --filter "ModelIntegrationTests"
# Result: 13 passed, 0 failed (includes ASR tests)
```

### Key Test Cases

1. **Varying Audio Lengths:** 0.5s, 1.0s, 2.0s, 3.0s, 5.0s - All pass ✅
2. **Edge Case Samples:** 160, 320, 800, 1600, 16000, 98348 - All pass ✅
3. **Multiple Calls:** 5 consecutive calls - Stable ✅
4. **Real Audio:** Integration test with `bruno_question.wav` - Pass ✅

## Technical Details

### Mel-Spectrogram to Audio Samples Relationship

```
Hop Length: 160 samples (10ms at 16kHz)
Window Length: 400 samples (25ms at 16kHz)

Mel Frames = ceil(Audio Samples / Hop Length)

Example:
- Audio: 16000 samples (1 second)
- Mel Frames: ceil(16000 / 160) = 100 frames
- Length Parameter: 16000 (not 100!)
```

### ONNX Model Signature

From `encoder.onnx`:

```python
Inputs:
  audio_signal: [batch, 128, time_frames]  # Mel-spectrogram
  length: [batch]                           # Raw audio sample count

Outputs:
  outputs: [batch, 1024, subsampled_time]  # Encoder output
  encoded_lengths: [batch]                  # Output sequence lengths
```

### Internal Subsampling

The model performs internal subsampling via convolutions:

```
Output Length = floor(floor(floor(T/2 - 1/2)/2)/2) + 1
```

The `length` parameter helps the model correctly:

1. Calculate positional embeddings
2. Generate attention masks
3. Handle variable-length sequences

## Configuration

Created `appsettings.json`:

```json
{
  "ModelConfig": {
    "AsrModelPath": "Models/parakeet-tdt-0.6b",
    "UseGpu": true
  }
}
```

## Testing Instructions

### 1. Run Unit Tests

```bash
dotnet test --filter "AsrServiceTests"
```

### 2. Run Integration Tests

```bash
dotnet test --filter "ModelIntegrationTests"
```

### 3. Test with Application

```bash
cd NvidiaVoiceAgent
dotnet run
```

Then:

1. Open browser to `http://localhost:5000`
2. Click microphone button
3. Speak into microphone
4. Verify transcription appears without errors

### Expected Behavior

**Before Fix:**

```
[Transcription error: [ErrorCode:RuntimeException] ... 
Attempting to broadcast an axis by a dimension other than 1. 39 by 77]
```

**After Fix:**

```
Transcript: "hello this is a test"  # Actual transcription
```

## Files Changed

1. ✅ `NvidiaVoiceAgent.Core/Services/AsrService.cs` - Fixed length calculation
2. ✅ `NvidiaVoiceAgent/appsettings.json` - Created configuration file
3. ✅ `tests/NvidiaVoiceAgent.Core.Tests/AsrServiceTests.cs` - Created comprehensive tests

## Lessons Learned

1. **ONNX Model Documentation:** Always inspect model metadata with `onnx.load()` to understand expected inputs
2. **Length Parameters:** Audio models often need original sample counts, not feature frame counts
3. **Test Coverage:** Edge cases (specific sample counts) are critical for audio processing
4. **Debug Logging:** Comprehensive logging of tensor shapes prevents guesswork

## Prevention

The unit tests now validate:

- Multiple audio durations
- Edge case sample counts
- No dimension mismatch errors
- Stable repeated calls

If the tests pass, the application should work correctly with real user speech.

---

**Status:** ✅ Fixed and Validated  
**Tests:** 24/24 passing (11 unit + 13 integration)
