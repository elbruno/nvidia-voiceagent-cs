# Real-Time Conversation Mode Design Plan

**Date:** February 17, 2026 | **Time:** 14:00

## Executive Summary

This plan outlines the implementation of a **Real-Time Continuous Conversation Mode** that will transform the NVIDIA Voice Agent from a single-turn ASR→LLM→TTS pipeline into an always-listening voice assistant that processes audio in real-time, detects pauses, and provides immediate responses.

**Current State:** Users must send complete audio chunks and wait for full processing.  
**Target State:** Users speak naturally with the microphone always open; responses stream back during pauses.

---

## Current Architecture Review

### Pipeline Components

1. **AsrService** - Parakeet-TDT-0.6B-V2 ASR model (~2.48 GB)
   - Input: 16kHz mono PCM float samples
   - Output: Text transcript
   - Throughput: ~0.5-1.0x realtime

2. **ILlmService** - PersonaPlex-7B LLM model (~16.7 GB)
   - Supports: GenerateResponseAsync() with chat history
   - Chat history context available: up to unlimited messages
   - Throughput: ~20-50 tokens/sec on GPU

3. **ITtsService** - FastPitch-HiFiGAN (NOT YET IMPLEMENTED)
   - Placeholder structure exists
   - No model downloaded or integrated
   - Key dependency for streaming audio responses

4. **AudioProcessor** - WAV encoding/decoding, resampling
   - Current use: One-time batch processing per message
   - Limitation: No real-time streaming support

### WebSocket Flow (Current)

```
Client → Audio Chunks (one-shot) → VoiceWebSocketHandler
         ↓
         Audio Decode → ASR → LLM → TTS → Response (one-shot)
         ↓
Response → JSON with base64-encoded audio → Client
```

---

## Proposed Real-Time Conversation Architecture

### New Component: AudioStreamBuffer

**Purpose:** Accumulate incoming audio and emit events on pause detection

```csharp
public class AudioStreamBuffer
{
    private float[] _buffer = new float[512000]; // ~32s @ 16kHz
    private int _writePos = 0;
    private DateTime _lastAudioTime;
    private int _pauseThreshold = 800; // 50ms pause threshold
    
    public event EventHandler<AudioChunkEventArgs>? OnAudioChunk;
    public event EventHandler? OnPauseDetected;
    
    public void AddAudioSamples(float[] samples) { /* ... */ }
    public float[] GetPendingAudio() { /* returns and clears buffer */ }
}
```

### New Component: VoiceActivityDetector (VAD)

**Purpose:** Detect speech/silence boundaries with minimal latency

```csharp
public interface IVoiceActivityDetector
{
    /// <summary>
    /// Analyze audio chunk and return confidence that speech is present (0.0-1.0)
    /// </summary>
    float GetSpeechConfidence(float[] audioSamples);
    
    /// <summary>
    /// Get recommended pause threshold in milliseconds
    /// </summary>
    int GetPauseThresholdMs { get; }
}
```

**Implementation Option 1:** Lightweight model-based VAD

- Use Silero VAD (small ONNX model, ~1 MB)
- Pros: Accurate, minimal overhead
- Cons: Adds another model dependency

**Implementation Option 2:** Energy-based heuristic VAD

- Analyze audio energy and spectral features
- Pros: No model dependency, very fast
- Cons: Less robust to background noise

**Recommendation:** Start with Option 2 (energy-based) for MVP, migrate to Silero VAD in Phase 2.

---

## Implementation Plan

### Phase 1: Audio Streaming Foundation (Week 1)

**Goal:** Enable continuous audio input with pause detection

#### 1.1 Extend VoiceSessionState

```csharp
public class VoiceSessionState
{
    public bool SmartMode { get; set; } = false;
    public string SmartModel { get; set; } = "phi3";
    public List<ChatMessage> ChatHistory { get; set; } = new();
    
    // NEW: Real-time conversation mode
    public bool RealtimeMode { get; set; } = false;
    public int PauseThresholdMs { get; set; } = 800; // 50ms default
    public DateTime LastAudioReceivedAt { get; set; } = DateTime.UtcNow;
    public float[] PendingAudioBuffer { get; set; } = new float[512000];
    public int PendingAudioLength { get; set; } = 0;
}
```

#### 1.2 Create AudioStreamBuffer Service

**File:** `NvidiaVoiceAgent.Core/Services/AudioStreamBuffer.cs`

```csharp
public class AudioStreamBuffer
{
    private readonly ILogger<AudioStreamBuffer> _logger;
    private float[] _buffer = new float[512000]; // ~32s @ 16kHz
    private int _writePos = 0;
    private DateTime _lastAudioTime = DateTime.UtcNow;
    private readonly int _pauseThresholdMs;
    
    public event EventHandler<AudioChunkEventArgs>? OnAudioChunk;
    public event EventHandler? OnPauseDetected;
    
    public AudioStreamBuffer(int pauseThresholdMs = 800)
    {
        _pauseThresholdMs = pauseThresholdMs;
    }
    
    public void AddSamples(float[] samples)
    {
        // Check for pause condition
        var timeSinceLastAudio = DateTime.UtcNow - _lastAudioTime;
        if (timeSinceLastAudio.TotalMilliseconds >= _pauseThresholdMs)
        {
            OnPauseDetected?.Invoke(this, EventArgs.Empty);
        }
        
        // Append to buffer, handle overflow
        Array.Copy(samples, 0, _buffer, _writePos, samples.Length);
        _writePos += samples.Length;
        _lastAudioTime = DateTime.UtcNow;
        
        OnAudioChunk?.Invoke(this, new AudioChunkEventArgs { Samples = samples });
    }
    
    public float[] GetAndClear()
    {
        var result = _buffer.Take(_writePos).ToArray();
        _writePos = 0;
        return result;
    }
}
```

**File:** `NvidiaVoiceAgent.Core/Models/AudioChunkEventArgs.cs`

```csharp
public class AudioChunkEventArgs : EventArgs
{
    public float[] Samples { get; set; } = Array.Empty<float>();
    public int SampleRate { get; set; } = 16000;
}
```

#### 1.3 Create Voice Activity Detector (Energy-Based for MVP)

**File:** `NvidiaVoiceAgent.Core/Services/IVoiceActivityDetector.cs`

```csharp
public interface IVoiceActivityDetector
{
    /// <summary>
    /// Detect if audio chunk contains speech
    /// </summary>
    /// <returns>Confidence 0.0 (silence) to 1.0 (definite speech)</returns>
    float AnalyzeAudio(float[] samples);
    
    /// <summary>
    /// Get energy-based silence threshold (0-1 scale)
    /// </summary>
    float SilenceThreshold { get; set; }
}
```

**File:** `NvidiaVoiceAgent.Core/Services/EnergyBasedVad.cs`

```csharp
public class EnergyBasedVad : IVoiceActivityDetector
{
    private readonly ILogger<EnergyBasedVad> _logger;
    public float SilenceThreshold { get; set; } = 0.02f; // Configurable threshold
    
    public EnergyBasedVad(ILogger<EnergyBasedVad> logger)
    {
        _logger = logger;
    }
    
    public float AnalyzeAudio(float[] samples)
    {
        if (samples.Length == 0) return 0.0f;
        
        // Calculate RMS energy
        float sumSquares = 0;
        for (int i = 0; i < samples.Length; i++)
        {
            sumSquares += samples[i] * samples[i];
        }
        float rmsEnergy = (float)Math.Sqrt(sumSquares / samples.Length);
        
        // Normalize to 0-1 range (typical speech energy: 0.01-0.5)
        float confidence = Math.Min(1.0f, rmsEnergy / 0.1f);
        
        return rmsEnergy > SilenceThreshold ? confidence : 0.0f;
    }
}
```

#### 1.4 Register New Services in Program.cs

```csharp
builder.Services.AddSingleton<IVoiceActivityDetector, EnergyBasedVad>();

// In AddVoiceAgentCore() extension method:
services.AddSingleton<AudioStreamBuffer>();
```

#### 1.5 Update VoiceWebSocketHandler.HandleAsync()

```csharp
private AudioStreamBuffer? _streamBuffer;
private IVoiceActivityDetector? _vad;

public async Task HandleAsync(WebSocket webSocket, CancellationToken cancellationToken)
{
    _logger.LogInformation("Voice WebSocket connection established");
    
    var sessionState = new VoiceSessionState();
    _streamBuffer = new AudioStreamBuffer();
    
    // Subscribe to pause events when realtime mode is active
    _streamBuffer.OnPauseDetected += async (s, e) =>
    {
        if (sessionState.RealtimeMode && _streamBuffer != null)
        {
            var pendingAudio = _streamBuffer.GetAndClear();
            if (pendingAudio.Length > 0)
            {
                await ProcessAudioChunkAsync(webSocket, pendingAudio, sessionState, cancellationToken);
            }
        }
    };
    
    // ... rest of loop ...
    
    // For binary messages (audio) in realtime mode:
    if (sessionState.RealtimeMode)
    {
        _streamBuffer.AddSamples(DecodeWavAudio(audioData));
        // Wait for pause detection to trigger processing
    }
}
```

---

### Phase 2: Streaming ASR & Response Generation (Week 2)

**Goal:** Enable partial transcriptions and streaming LLM responses

#### 2.1 Streaming ASR Support

**Challenge:** Current Parakeet-TDT model is encoder-only with CTC decoding, which requires complete audio before decent results.

**Solution Options:**

| Option | Pros | Cons |
|--------|------|------|
| **Option A:** Use Whisper-based models | Streaming capable, better accuracy | Larger models (harder to quantize) |
| **Option B:** Chunked CTC decoding | Works with current model | Partial results quality issues |
| **Option C:** Hybrid approach | Best of both | Complex implementation |

**Recommendation for MVP:** Option B using current model

- Configure smaller audio chunks (1-2 seconds)
- Implement confidence-based partial results
- Allow refinement on complete audio

**Interface Update:**

```csharp
public interface IAsrService
{
    /// Existing:
    Task<string> TranscribeAsync(float[] audioSamples, CancellationToken cancellationToken = default);
    
    /// New for streaming:
    Task<(string transcript, float confidence)> TranscribePartialAsync(
        float[] audioSamples,
        CancellationToken cancellationToken = default);
}
```

#### 2.2 Streaming LLM Support

**Challenge:** PersonaPlex currently only exposes GenerateResponseAsync() which returns complete response.

**Solution:** Implement token-by-token streaming in ILlmService

```csharp
public interface ILlmService
{
    // Existing:
    Task<string> GenerateResponseAsync(string prompt, CancellationToken cancellationToken = default);
    
    // New for streaming:
    IAsyncEnumerable<string> GenerateResponseStreamAsync(
        string prompt,
        List<ChatMessage> chatHistory,
        CancellationToken cancellationToken = default);
}
```

**Implementation:**

```csharp
public async IAsyncEnumerable<string> GenerateResponseStreamAsync(
    string prompt,
    List<ChatMessage> chatHistory,
    [EnumeratorCancellation] CancellationToken cancellationToken = default)
{
    var fullResponse = "";
    var tokens = new List<string>();
    
    // Use existing GenerateResponseAsync but yield tokens as generated
    // If PersonaPlex doesn't support token streaming natively:
    // - Generate full response
    // - Split into word/phrase chunks
    // - Yield with minimal latency
    
    var response = await GenerateResponseAsync(prompt, cancellationToken);
    
    // Yield by word boundaries for natural streaming
    var words = response.Split(' ');
    foreach (var word in words)
    {
        fullResponse += word + " ";
        yield return fullResponse;
        await Task.Delay(50); // Simulate streaming delay (tunable)
    }
}
```

---

### Phase 3: Streaming TTS & Audio Output (Week 3)

**Goal:** Stream audio responses in real-time

#### 3.1 Implement TtsService with Streaming Support

**Current Status:** TtsService interface exists but not implemented

**New File:** `NvidiaVoiceAgent.Core/Services/TtsService.cs`

```csharp
public class TtsService : ITtsService
{
    private readonly ILogger<TtsService> _logger;
    private InferenceSession? _fastpitchSession;
    private InferenceSession? _hifiganSession;
    private bool _isModelLoaded;
    
    public bool IsModelLoaded => _isModelLoaded;
    
    public async Task LoadModelAsync(CancellationToken cancellationToken = default)
    {
        // Load FastPitch encoder for mel-spectrogram generation
        // Load HiFiGAN vocoder for waveform synthesis
    }
    
    /// <summary>
    /// Synthesize full audio from text
    /// </summary>
    public async Task<byte[]> SynthesizeAsync(string text, CancellationToken cancellationToken = default)
    {
        // Convert text → phonemes → mel-spectrogram → waveform
        var melSpec = await GenerateMelSpectrogramAsync(text, cancellationToken);
        var waveform = await VocoderAsync(melSpec, cancellationToken);
        return EncodeWavAudio(waveform);
    }
    
    /// <summary>
    /// Stream audio chunks as text is synthesized
    /// </summary>
    public async IAsyncEnumerable<byte[]> SynthesizeStreamAsync(
        IAsyncEnumerable<string> textTokens,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        var accumulatedText = "";
        
        await foreach (var token in textTokens)
        {
            accumulatedText += token;
            
            // Once we have a complete phrase (e.g., sentence), synthesize
            if (token.EndsWith(".") || token.EndsWith("!") || token.EndsWith("?"))
            {
                var audioChunk = await SynthesizeAsync(accumulatedText, cancellationToken);
                yield return audioChunk;
                accumulatedText = "";
            }
        }
        
        // Synthesize any remaining text
        if (!string.IsNullOrEmpty(accumulatedText))
        {
            var audioChunk = await SynthesizeAsync(accumulatedText, cancellationToken);
            yield return audioChunk;
        }
    }
    
    private async Task<float[,]> GenerateMelSpectrogramAsync(
        string text,
        CancellationToken cancellationToken = default)
    {
        // FastPitch inference:
        // Input: token IDs
        // Output: mel-spectrogram [mel_bins × time]
    }
    
    private async Task<float[]> VocoderAsync(
        float[,] melSpec,
        CancellationToken cancellationToken = default)
    {
        // HiFiGAN inference:
        // Input: mel-spectrogram
        // Output: waveform samples @ 22050 Hz
    }
}
```

#### 3.2 WebSocket Response Streaming

**New Response Types:**

```csharp
public class PartialTranscriptResponse
{
    public string Transcript { get; set; } = "";
    public float Confidence { get; set; } = 0.0f;
    public bool IsPartial { get; set; } = true;
}

public class PartialLlmResponse
{
    public string Text { get; set; } = "";
    public bool IsComplete { get; set; } = false;
}

public class AudioStreamChunk
{
    /// <summary>Base64-encoded WAV audio chunk</summary>
    public string AudioBase64 { get; set; } = "";
    
    /// <summary>Is this the final chunk?</summary>
    public bool IsFinal { get; set; } = false;
}
```

**Updated Handler Flow:**

```csharp
private async Task ProcessAudioChunkAsync(
    WebSocket webSocket,
    float[] audioSamples,
    VoiceSessionState sessionState,
    CancellationToken cancellationToken)
{
    try
    {
        // 1. Stream partial transcription
        var (transcript, confidence) = await _asrService.TranscribePartialAsync(audioSamples, cancellationToken);
        await SendJsonAsync(webSocket, new PartialTranscriptResponse
        {
            Transcript = transcript,
            Confidence = confidence,
            IsPartial = true
        }, cancellationToken);
        
        // 2. Stream LLM response tokens
        await SendJsonAsync(webSocket, new ThinkingResponse(), cancellationToken);
        
        var llmTokens = _llmService.GenerateResponseStreamAsync(
            transcript, sessionState.ChatHistory, cancellationToken);
        
        string fullResponse = "";
        await foreach (var token in llmTokens)
        {
            fullResponse = token;
            await SendJsonAsync(webSocket, new PartialLlmResponse
            {
                Text = token,
                IsComplete = false
            }, cancellationToken);
        }
        
        // 3. Stream TTS audio
        var ttsTokens = GenerateTtsTokenStream(fullResponse);
        
        await foreach (var audioChunk in _ttsService.SynthesizeStreamAsync(ttsTokens, cancellationToken))
        {
            await SendJsonAsync(webSocket, new AudioStreamChunk
            {
                AudioBase64 = Convert.ToBase64String(audioChunk),
                IsFinal = false
            }, cancellationToken);
        }
        
        // 4. Final response
        await SendJsonAsync(webSocket, new PartialLlmResponse
        {
            Text = fullResponse,
            IsComplete = true
        }, cancellationToken);
        
        // Update chat history
        sessionState.ChatHistory.Add(new ChatMessage { Role = "user", Content = transcript });
        sessionState.ChatHistory.Add(new ChatMessage { Role = "assistant", Content = fullResponse });
    }
    catch (Exception ex)
    {
        _logger.LogError(ex, "Error processing audio chunk");
        await BroadcastLogAsync($"Error processing audio: {ex.Message}", "error");
    }
}
```

---

### Phase 4: Client-Side Integration (Week 4)

**Goal:** Update browser UI to support real-time conversation

#### 4.1 Updated WebSocket Protocol

**New Configuration Message:**

```json
{
    "type": "config",
    "realtimeMode": true,
    "pauseThresholdMs": 800,
    "smartMode": true,
    "smartModel": "personaplex"
}
```

**Response Messages (Client receives):**

1. `PartialTranscriptResponse` - Partial ASR result
2. `PartialLlmResponse` - Streaming LLM tokens
3. `AudioStreamChunk` - Streaming audio data
4. Original `VoiceResponse` - Final complete response (for compatibility)

#### 4.2 Browser UI Changes (wwwroot/index.html)

```html
<!-- New Controls -->
<div class="controls">
    <label>
        <input type="checkbox" id="realtimeMode" /> Realtime Conversation Mode
    </label>
    <label>
        Pause Threshold: <input type="range" id="pauseThreshold" min="300" max="2000" value="800" /> ms
    </label>
</div>

<div class="transcript-stream" id="transcriptStream"></div>
<div class="response-stream" id="responseStream"></div>
<div class="audio-player" id="audioPlayer"></div>
```

---

## Technical Considerations

### Audio Buffering Strategy

```
Microphone → Audio Chunks (100ms) → AudioStreamBuffer
                                         ↓ On Pause
                                    Emit Pending Audio
                                         ↓
                                    ASR (partial + full)
                                    LLM (streaming)
                                    TTS (streaming)
                                         ↓
                                    Stream to Client
```

### Pause Detection Algorithm

```csharp
private bool IsAudioPause(TimeSpan timeSinceLastAudio, float recentEnergy)
{
    // Multi-factor pause detection:
    return timeSinceLastAudio > PauseThreshold 
        && recentEnergy < SilenceThreshold;
}
```

**Recommended Pause Threshold:** 800ms (tunable in config)

- Detects natural speech pauses
- Avoids false positives from quick consonants
- Feels responsive to user

### Performance Targets

| Component | Target | Notes |
|-----------|--------|-------|
| Audio buffering | <50ms latency | Store-and-forward |
| ASR processing | <1s for 2s audio | Current model: ~0.5-1.0x realtime |
| LLM generation | 20-50 tokens/s | PersonaPlex on GPU |
| TTS synthesis | 4-6x realtime | FastPitch + HiFiGAN |
| Client display | <100ms | Stream updates |

### Memory Management

- **Audio Buffer:** 32 seconds @ 7.2 KB/s = ~230 MB (reasonable for 16GB+)
- **ASR Session:** ~500 MB (model) + ~50 MB (runtime)
- **LLM Session:** ~8 GB (PersonaPlex quantized)
- **Chat History:** Configurable max messages (default: 50)

---

## Configuration Files

### appsettings.Development.json Updates

```json
{
  "ModelConfig": {
    "UseGpu": true,
    "AsrModelPath": "models/parakeet-tdt-0.6b",
    "LlmModelPath": "models/personaplex-7b"
  },
  "VoiceAgent": {
    "RealtimeMode": {
      "Enabled": true,
      "DefaultPauseThresholdMs": 800,
      "MaxAudioBufferSeconds": 32,
      "SilenceThreshold": 0.02
    },
    "StreamingOptions": {
      "EnableAsterStreaming": true,
      "EnableLLMStreaming": true,
      "EnableTTSStreaming": true,
      "ChunkSizeMs": 100
    }
  }
}
```

---

## Implementation Checklist

### Phase 1 (Week 1): Foundation

- [ ] Create AudioStreamBuffer class
- [ ] Implement EnergyBasedVad
- [ ] Update VoiceSessionState
- [ ] Register new services
- [ ] Update VoiceWebSocketHandler pause detection
- [ ] Test pause detection accuracy

### Phase 2 (Week 2): Streaming ASR & LLM

- [ ] Implement IAsrService.TranscribePartialAsync()
- [ ] Implement ILlmService.GenerateResponseStreamAsync()
- [ ] Add streaming response types
- [ ] Update WebSocket handler for streaming LLM
- [ ] Test partial transcription quality

### Phase 3 (Week 3): Streaming TTS

- [ ] Implement TtsService.cs
- [ ] Load FastPitch + HiFiGAN models
- [ ] Implement SynthesizeStreamAsync()
- [ ] Test audio streaming latency
- [ ] Handle audio discontinuities

### Phase 4 (Week 4): Client Integration

- [ ] Update browser UI for realtime mode
- [ ] Add streaming response display
- [ ] Test end-to-end conversation flow
- [ ] Benchmark performance
- [ ] Optimization & refinement

---

## Testing Strategy

### Unit Tests

```csharp
[Fact]
public void AudioStreamBuffer_OnPauseGap_EmitsPauseEvent()
{
    var buffer = new AudioStreamBuffer(pauseThresholdMs: 500);
    var pauseFired = false;
    buffer.OnPauseDetected += (s, e) => pauseFired = true;
    
    // Add audio, wait past threshold
    buffer.AddSamples(GenerateTestAudio(100));
    Task.Delay(600).Wait();
    buffer.AddSamples(GenerateTestAudio(100)); // Should trigger pause
    
    Assert.True(pauseFired);
}
```

### Integration Tests

- Record real microphone audio with pauses
- Verify pause detection triggers at correct times
- Measure end-to-end latency
- Test with various audio qualities

### Load Tests  

- Multiple simultaneous conversations
- Memory usage under sustained streaming
- CPU/GPU utilization

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|-----------|
| **Pause threshold too sensitive** | False triggers, choppy output | Config tuning window (300-2000ms) |
| **Streaming LLM OOM** | App crash | Limit chat history, implement cleanup |
| **Audio buffer overflow** | Lost audio | Implement circular buffer, log warnings |
| **TTS not ready** | Phase 3 delayed | Pre-implement stubs, test async only |
| **PersonaPlex streaming latency** | Feels sluggish | Pre-word token batching optimization |

---

## Success Criteria

✅ **Pause detection:** <100ms false positive rate in quiet environment  
✅ **End-to-end latency:** <3 seconds from pause detection to audio output start  
✅ **User experience:** Feels like natural conversation, not robotic  
✅ **Stability:** No crashes during 30-minute continuous conversation  
✅ **Memory:** <12 GB peak usage for extended sessions  

---

## Future Enhancements (Phase 5+)

- **Interruption Handling:** Detect user trying to speak while AI is responding
- **Voice Cloning:** Generate responses in user's voice (PersonaPlex feature)
- **Real-time Translation:** Multilingual realtime conversation
- **Acoustic Echo Cancellation:** Better audio quality for speaker playback
- **Context Awareness:** Incorporate room audio, environmental context
- **Model Optimization:** Quantized versions for CPU-only systems

---

## References

- **Parakeet-TDT:** <https://github.com/NVIDIA/NeMo/blob/main/examples/tts/fastpitch.py>
- **PersonaPlex:** <https://catalog.ngc.nvidia.com/orgs/nvidia/models/personaplex_7b>
- **FastPitch + HiFiGAN:** NVIDIA Real-time Text-to-Speech synthesis models
- **Voice Activity Detection:** Silero VAD (<https://github.com/snakers4/silero-vad>)
- **WebSocket Streaming:** <https://datatracker.ietf.org/doc/html/rfc6455>

---

**Created by:** GitHub Copilot AI Assistant  
**Repository:** nvidia-voiceagent-cs  
**Status:** Draft - Ready for team review
