# PersonaPlex-7B-v1 Integration Plan
**Date:** 2026-02-17  
**Status:** In Progress

## Overview
Implement support for NVIDIA's PersonaPlex-7B-v1 model in the C# voice agent application. PersonaPlex is a full-duplex, speech-to-speech conversational AI model that can handle real-time conversations with persona and voice control.

## Model Specifications

### PersonaPlex-7B-v1 Details
- **Repository:** `nvidia/personaplex-7b-v1` (HuggingFace - Gated Access)
- **Type:** Full-duplex speech-to-speech model
- **Size:** 7 billion parameters (~16.7 GB for model.safetensors)
- **Architecture:** Based on Moshi (Unified Dual-Stream Transformer)
- **Format:** SafeTensors (PyTorch), NOT ONNX
- **License:** NVIDIA Open Model License (gated, requires acceptance)

### Key Files Required
1. **model.safetensors** (~16.7 GB) - Main model checkpoint
2. **tokenizer-e351c8d8-checkpoint125.safetensors** (~385 MB) - Tokenizer weights
3. **tokenizer_spm_32k_3.model** (~553 KB) - SentencePiece tokenizer
4. **voices.tgz** (6.1 MB) - Pre-packaged voice embeddings (18 voices)

### Technical Features
- **Full Duplex:** Simultaneous listening and speaking
- **Ultra-Low Latency:** ~170ms TTFT, ~240ms interruption response
- **Voice + Text Prompting:** Voice defines timbre/prosody, text defines persona/role
- **Speech Codec:** Mimi Speech Encoder (operates at 24kHz)
- **GPU Requirements:** Recommended NVIDIA GPU, minimum 4GB VRAM with quantization

## Current Architecture Analysis

### Existing Pipeline
Current voice agent flow: **Audio → ASR → LLM → TTS → Audio**
- **ASR (Parakeet-TDT):** Speech-to-text (ONNX)
- **LLM (Planned):** Text generation - TinyLlama/Phi-3 (ONNX)
- **TTS (Planned):** Text-to-speech - FastPitch + HiFiGAN (ONNX)

### PersonaPlex Integration Challenges
1. **Format Mismatch:** PersonaPlex is PyTorch/SafeTensors, not ONNX
2. **Pipeline Difference:** PersonaPlex is end-to-end speech-to-speech, bypassing separate ASR/LLM/TTS
3. **Runtime Requirement:** Needs PyTorch runtime or ONNX conversion (complex for 7B transformer)
4. **Model Size:** 16.7 GB is large compared to current models (~2.5 GB for Parakeet)

## Implementation Options

### Option A: Full PersonaPlex Integration (Complex)
**Goal:** Replace ASR → LLM → TTS pipeline with PersonaPlex end-to-end

**Requirements:**
- Convert PersonaPlex to ONNX format (challenging for 7B model)
- OR integrate PyTorch.NET / TorchSharp runtime
- Implement Mimi audio codec in C#
- Handle voice embedding management
- Support streaming audio processing

**Pros:**
- True full-duplex capability
- Lower latency than current pipeline
- High-quality voice control

**Cons:**
- Major architectural change
- PyTorch dependency (TorchSharp already in project but not fully utilized)
- Complex ONNX conversion for 7B model
- Significant development effort

### Option B: PersonaPlex as LLM Alternative (Pragmatic)
**Goal:** Use PersonaPlex as a smart LLM service, keeping ASR/TTS separate

**Requirements:**
- Add PersonaPlex to ModelRegistry for download
- Create ILlmService implementation using TorchSharp
- Convert text prompt to audio internally (use existing TTS mock)
- Extract text from PersonaPlex audio output (use existing ASR)
- Keep existing WebSocket pipeline flow

**Pros:**
- Minimal changes to existing architecture
- Leverages existing ASR/TTS infrastructure
- Uses TorchSharp (already in dependencies)
- Can coexist with traditional LLM implementations

**Cons:**
- Not utilizing full PersonaPlex capabilities (duplex, voice control)
- Double audio encoding/decoding overhead
- Loses ultra-low latency benefits

### Option C: Hybrid Approach (Recommended)
**Goal:** Add PersonaPlex alongside existing pipeline with mode selection

**Requirements:**
1. **Download Infrastructure:**
   - Add PersonaPlex model to ModelRegistry
   - Support SafeTensors download via ModelHub
   - Handle gated repo authentication

2. **Service Implementation:**
   - Create `PersonaPlexService` implementing `ILlmService`
   - Use TorchSharp for model loading/inference
   - Implement text-to-speech-to-text wrapper initially
   - Plan for future direct speech-to-speech mode

3. **Configuration:**
   - Add PersonaPlex config to appsettings.json
   - Support mode selection (Traditional vs PersonaPlex)
   - Voice embedding selection

4. **WebSocket Handler:**
   - Extend VoiceWebSocketHandler to support PersonaPlex mode
   - Add voice persona configuration messages
   - Support streaming for future duplex mode

**Pros:**
- Incremental implementation path
- Maintains backward compatibility
- Enables future full-duplex upgrade
- User can choose between modes

**Cons:**
- More complex codebase
- Multiple code paths to maintain

## Recommended Implementation Plan

### Phase 1: Download Infrastructure (Immediate)
- [ ] Add `ModelType.PersonaPlex` to enum
- [ ] Register PersonaPlex model in ModelRegistry
- [ ] Configure HuggingFace gated repo support
- [ ] Test model download with authentication
- [ ] Update UI to show PersonaPlex availability

### Phase 2: Basic LLM Service (Core)
- [ ] Create `PersonaPlexService.cs` implementing `ILlmService`
- [ ] Implement SafeTensors loading with TorchSharp
- [ ] Create text-to-text inference wrapper
- [ ] Add voice embedding management
- [ ] Mock mode for development without model
- [ ] Unit tests for PersonaPlexService

### Phase 3: Integration (Testing)
- [ ] Register PersonaPlexService in DI container
- [ ] Update VoiceWebSocketHandler to use PersonaPlex
- [ ] Add configuration for model selection
- [ ] Test end-to-end voice pipeline
- [ ] Performance benchmarking

### Phase 4: Advanced Features (Future)
- [ ] Direct speech-to-speech mode (bypass ASR/TTS)
- [ ] Streaming audio processing
- [ ] Voice cloning support
- [ ] Full-duplex interruption handling
- [ ] Multiple voice persona selection

## Technical Decisions

### Framework Choice: TorchSharp
**Rationale:**
- Already in project dependencies (`PackageReference Include="TorchSharp" Version="0.102.6"`)
- Native .NET bindings to libtorch
- Supports SafeTensors loading
- GPU acceleration support
- Active development and community

### Alternative Considered: ONNX Runtime
**Issues:**
- No official ONNX export for PersonaPlex
- 7B model conversion is complex
- May lose duplex capabilities
- Requires custom export pipeline

### SafeTensors Support
**Solution:**
- Use TorchSharp's SafeTensors loading APIs
- Or use `safetensors-dotnet` NuGet package if needed
- Handle memory-mapped files for large models

## File Structure Changes

```
NvidiaVoiceAgent.Core/
├── Services/
│   ├── PersonaPlexService.cs        (NEW)
│   └── IPersonaPlexService.cs       (NEW - optional, if different from ILlmService)
├── Models/
│   ├── PersonaPlexConfig.cs         (NEW)
│   └── VoiceEmbedding.cs            (NEW)

NvidiaVoiceAgent.ModelHub/
├── ModelType.cs                      (MODIFIED - add PersonaPlex)
└── ModelRegistry.cs                  (MODIFIED - add PersonaPlex model info)

NvidiaVoiceAgent/
├── appsettings.json                  (MODIFIED - add PersonaPlex config)
└── Program.cs                        (MODIFIED - register PersonaPlexService)
```

## Configuration Example

```json
{
  "ModelConfig": {
    "PersonaPlexModelPath": "model-cache/personaplex-7b",
    "PersonaPlexVoice": "voice_0",
    "UsePersonaPlex": false
  },
  "ModelHub": {
    "HuggingFaceToken": "hf_your_token_here"
  }
}
```

## Testing Strategy

1. **Unit Tests:**
   - PersonaPlexService initialization
   - SafeTensors loading
   - Text-to-text inference wrapper
   - Voice embedding selection
   - Mock mode operation

2. **Integration Tests:**
   - Model download from gated repo
   - Service registration in DI
   - WebSocket communication
   - End-to-end voice pipeline

3. **Performance Tests:**
   - Model loading time
   - Inference latency
   - Memory usage (16GB+ model)
   - GPU utilization

## Risks and Mitigations

### Risk: Model Size (16.7 GB)
**Mitigation:**
- Support quantization (INT8, INT4)
- Memory-mapped model loading
- Clear system requirements documentation

### Risk: PyTorch Runtime Complexity
**Mitigation:**
- Use stable TorchSharp version
- Extensive error handling
- Graceful fallback to mock mode
- Document installation requirements

### Risk: Gated Repo Access
**Mitigation:**
- Clear documentation for HuggingFace token setup
- Helpful error messages
- Optional: Support local model path

### Risk: Performance on CPU
**Mitigation:**
- Recommend GPU for production
- Support CPU inference with clear latency expectations
- Consider smaller model variants if available

## Success Criteria

1. ✅ Model downloads successfully from gated HuggingFace repo
2. ✅ PersonaPlexService loads model using TorchSharp
3. ✅ Text-to-text inference produces coherent responses
4. ✅ Integration with VoiceWebSocketHandler works end-to-end
5. ✅ Performance is acceptable (< 2s response time on GPU)
6. ✅ All tests pass
7. ✅ Documentation is complete

## Open Questions

1. **ONNX Conversion:** Should we attempt ONNX export for consistency? 
   - **Decision:** No, use TorchSharp for Phase 1-2, revisit for Phase 4

2. **Voice Embeddings:** How to expose voice selection in UI?
   - **Decision:** Add voice selection to ConfigMessage

3. **Memory Requirements:** What's minimum system spec?
   - **Decision:** Document 20GB RAM + 8GB GPU VRAM minimum

4. **License Compliance:** How to handle gated model in production?
   - **Decision:** Document license requirements, require user acceptance

## References

- [PersonaPlex HuggingFace](https://huggingface.co/nvidia/personaplex-7b-v1)
- [PersonaPlex GitHub](https://github.com/NVIDIA/personaplex)
- [NVIDIA Research Page](https://research.nvidia.com/labs/adlr/personaplex/)
- [TorchSharp Documentation](https://github.com/dotnet/TorchSharp)
- [SafeTensors Format](https://github.com/huggingface/safetensors)

## Timeline Estimate

- **Phase 1 (Download):** 4-6 hours
- **Phase 2 (Service):** 8-12 hours
- **Phase 3 (Integration):** 4-6 hours
- **Phase 4 (Advanced):** Future work (20+ hours)

**Total for Phases 1-3:** ~16-24 hours development + testing

## Notes

- PersonaPlex represents a significant architectural shift from traditional ASR→LLM→TTS
- Incremental approach allows learning and validation at each step
- TorchSharp provides the necessary runtime without Python dependencies
- Can serve as foundation for future speech-to-speech models
